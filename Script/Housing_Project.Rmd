---
title: "Main_Script_Housing_Prices"
author: "Aditya Lahiri"
date: "March 28, 2019"
output: pdf_document
---
## Load the libraries
```{r}
library(ggplot2)
library(dplyr)
library(corrplot)
library(mice)
library(VIM)
```


## Load the data
```{r setup, include=FALSE}
train_df = read.csv("C:/Users/adi44/Desktop/ATM/Spring 2019/Stat689-603/Project/house-prices-advanced-regression-techniques/STAT689/Data/train.csv", stringsAsFactors = FALSE)

test_df=read.csv("C:/Users/adi44/Desktop/ATM/Spring 2019/Stat689-603/Project/house-prices-advanced-regression-techniques/STAT689/Data/test.csv", stringsAsFactors = FALSE)
```

## Data Preprocessing

Let's find out all the features in the training set
```{r}
str(train_df)
```

Remove the ID column from training and test set as it wont be used for modeling
```{r}
train_ID = train_df$Id
test_ID = test_df$Id
train_df=select(train_df,-c(1))
test_df=select(test_df,-c(1))
```

Analyze the dependent variable SalesPrice
```{r}
qplot(SalePrice, data = train_df, bins = 50, main = "Original Sales Price distribution: Left skewed ")

```

We need to fix the skew, so we log transform the dependent variable to make the
distribution closer to normal. 

```{r}
## Log transformation of the dependent variable
train_df$SalePrice <- log(train_df$SalePrice + 1)

## Normal distribution after transformation
qplot(SalePrice, data = train_df, bins = 50, main = "Sale Prices distribution after log transformation")
```

```{r}
identify_numeric<-array(dim=dim(train_df)[2])
for (i in 1:dim(train_df)[2]){
        identify_numeric[i]<-is.numeric(train_df[,i])
}
corrplot.name<-colnames(train_df)[identify_numeric][1:dim(train_df)[2]-1] ## exlude id, and saleprice
corrdata<-na.omit(train_df[, (names(train_df) %in% corrplot.name)])## remove missing for correlation plot
Corr_Mat <- cor(corrdata)
corrplot.mixed(Corr_Mat, lower="circle", upper="circle",tl.pos="lt", diag="n", order="hclust", hclust.method="complete",tl.cex=0.8,tl.offset = 0.5)

diag(Corr_Mat)<-0


```

```{r}

Corr_Mat[upper.tri(Corr_Mat)]<-0
Threshold_Corr <-0.75
High_Corr_Vars<-which(abs(Corr_Mat) > 0.75,arr.ind = TRUE)
print("The Variables with high correlations are: ")
cat("\n")
for (iter in 1:dim(High_Corr_Vars)[1]){
  
  Var_Name1<-rownames(Corr_Mat)[High_Corr_Vars[iter,1]]
  Var_name2<-colnames(Corr_Mat)[High_Corr_Vars[iter,2]]
  print(paste(Var_Name1,Var_name2, sep = " and "))
  cat("\n")
}
```
Since SalePrice is the dependent variable we cannot eliminate it. However, we 
can eliminate each of the independent variables from the pairs of highly correlated independent variables. We will eliminate GarageYrBlt, X1stFlrSF,TotRmsAbvGrd and GarageCars.It is necessary to remove highly correlated independent variable to prevent overfitting during model building.


```{r}
drop<-c("GarageYrBlt","X1stFlrSF","TotRmsAbvGrd","GarageCars")
train_df<-train_df %>% select(-one_of(drop))
test_df<-test_df %>% select(-one_of(drop))
dim(train_df)
dim(test_df)
```

We will now check the dataset for missing data. 
We will remove any features where majority of the instances are missing.
```{r}

NAcol_Train <- which(colSums(is.na(train_df)) > 0)
NAcol_Test<- which(colSums(is.na(test_df)) > 0)

cat('There are', length(NAcol_Train), 'columns with missing values in Training Set')

cat('\n There are', length(NAcol_Test), 'columns with missing values in Testing Set')

```


There may be some overlap among the features with missing values in the training and testing set. Therefore we will create a column for SalePrices with NA values in the testing set and then combine training and testing set. Once we have dealt with the missing data we will split the train/test set. 

```{r}
test_df$SalePrice <- NA
combined_data <- rbind(train_df, test_df)
NAcol<- which(colSums(is.na(combined_data)) > 0)
cat('There are', length(NAcol), 'columns with missing values in Training Set')

```

As suspected indeed there is some overlap of missing data among the features in the testing and training set. We will now clean the missing data.

Here's the percentage of missing data missing by variables. Variables with more than 15% missing data are removed from the model.
```{r}
aggr(combined_data,col=c('blue','red'),numbers=TRUE,sortVars=TRUE)
NAcombined=colSums(is.na(combined_data))/dim(combined_data)[1]
NAcombined=sort(subset(NAcombined,NAcombined>0),decreasing=TRUE)
plot(NAcombined,type="h",lwd=10,xaxt="n")
axis(1,at=1:length(NAcombined),labels=names(NAcombined))
cat("The variables that contain more than 50% missing data are",names(subset(NAcombined,NAcombined>0.15)))
combined_data_clean=subset(combined_data,select=-c(PoolQC,MiscFeature,Alley,Fence,FireplaceQu,LotFrontage))
```

Converting characters to factors for the imputation and modeling process.
```{r}
combined_data_clean=combined_data_clean%>%
  mutate_if(is.character,as.factor)
```
  
```{r}
temp_data=mice(subset(combined_data_clean,select=-SalePrice),m=1,maxit = 25,meth='pmm',seed=100)
summary(temp_data)
combined_data_imputed=cbind(complete(temp_data,1),combined_data_clean$SalePrice)
colnames(combined_data_imputed)[colnames(combined_data_imputed)=="combined_data_clean$SalePrice"] <- "SalePrice"
```

Separating train and test sets from the combined set.
```{r}
train_imputed=subset(combined_data_imputed,combined_data_imputed$SalePrice!="NA")
test_imputed=subset(combined_data_imputed,is.na(combined_data_imputed$SalePrice))

test_imputed =select(test_imputed,-one_of("SalePrice"))
```

Let's check the training set for any missing values
```{r}
cat('There are', length(which(colSums(is.na(train_imputed)) > 0)), 'columns with missing values in Training Set')
cat("\n")
cat('There are', length(which(colSums(is.na(test_imputed)) > 0)), 'columns with missing values in Testing Set')

```



