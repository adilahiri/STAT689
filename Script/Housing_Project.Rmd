---
title: "Main_Script_Housing_Prices"
author: "Aditya Lahiri"
date: "March 28, 2019"
output: pdf_document
---
## Load the libraries
```{r}
library(ggplot2)
library(dplyr)
library(corrplot)
library(mice)
library(VIM)
library(glmnet)
library(Metrics)
```


## Load the data
```{r setup, include=FALSE}
train_df = read.csv("C:/Users/adi44/Desktop/ATM/Spring 2019/Stat689-603/Project/house-prices-advanced-regression-techniques/STAT689/Data/train.csv", stringsAsFactors = FALSE)

test_df=read.csv("C:/Users/adi44/Desktop/ATM/Spring 2019/Stat689-603/Project/house-prices-advanced-regression-techniques/STAT689/Data/test.csv", stringsAsFactors = FALSE)
```

## Data Preprocessing

Let's find out all the features in the training set
```{r}
str(train_df)
```

Remove the ID column from training and test set as it wont be used for modeling
```{r}
train_ID = train_df$Id
test_ID = test_df$Id
train_df=select(train_df,-c(1))
test_df=select(test_df,-c(1))
```

Analyze the dependent variable SalesPrice
```{r}
qplot(SalePrice, data = train_df, bins = 50, main = "Original Sales Price distribution: Left skewed ")

```

We need to fix the skew, so we log transform the dependent variable to make the
distribution closer to normal. 

```{r}
## Log transformation of the dependent variable
train_df$SalePrice <- log(train_df$SalePrice + 1)

## Normal distribution after transformation
qplot(SalePrice, data = train_df, bins = 50, main = "Sale Prices distribution after log transformation")
```

```{r}
identify_numeric<-array(dim=dim(train_df)[2])
for (i in 1:dim(train_df)[2]){
        identify_numeric[i]<-is.numeric(train_df[,i])
}
corrplot.name<-colnames(train_df)[identify_numeric][1:dim(train_df)[2]-1] ## exlude id, and saleprice
corrdata<-na.omit(train_df[, (names(train_df) %in% corrplot.name)])## remove missing for correlation plot
Corr_Mat <- cor(corrdata)
corrplot.mixed(Corr_Mat, lower="circle", upper="circle",tl.pos="lt", diag="n", order="hclust", hclust.method="complete",tl.cex=0.8,tl.offset = 0.5)

diag(Corr_Mat)<-0


```

```{r}

Corr_Mat[upper.tri(Corr_Mat)]<-0
Threshold_Corr <-0.75
High_Corr_Vars<-which(abs(Corr_Mat) > 0.75,arr.ind = TRUE)
print("The Variables with high correlations are: ")
cat("\n")
for (iter in 1:dim(High_Corr_Vars)[1]){
  
  Var_Name1<-rownames(Corr_Mat)[High_Corr_Vars[iter,1]]
  Var_name2<-colnames(Corr_Mat)[High_Corr_Vars[iter,2]]
  print(paste(Var_Name1,Var_name2, sep = " and "))
  cat("\n")
}
```
Since SalePrice is the dependent variable we cannot eliminate it. However, we 
can eliminate each of the independent variables from the pairs of highly correlated independent variables. We will eliminate GarageYrBlt,TotRmsAbvGrd and GarageCars.It is necessary to remove highly correlated independent variable to prevent overfitting during model building. We do not remove X1stFlrSF as it is a surface area and combined with other areas to provide a
measure of total surface area.


```{r}
drop<-c("GarageYrBlt","TotRmsAbvGrd","GarageCars")
train_df<-train_df %>% select(-one_of(drop))
test_df<-test_df %>% select(-one_of(drop))
dim(train_df)
dim(test_df)
```

We will now check the dataset for missing data. 
We will remove any features where majority of the instances are missing.
```{r}

NAcol_Train <- which(colSums(is.na(train_df)) > 0)
NAcol_Test<- which(colSums(is.na(test_df)) > 0)

cat('There are', length(NAcol_Train), 'columns with missing values in Training Set')

cat('\nThere are', length(NAcol_Test), 'columns with missing values in Testing Set')

```


There may be some overlap among the features with missing values in the training and testing set. Therefore we will create a column for SalePrices with NA values in the testing set and then combine training and testing set. Once we have dealt with the missing data we will split the train/test set. 

```{r}
test_df$SalePrice <- NA
combined_data <- rbind(train_df, test_df)
NAcol<- which(colSums(is.na(combined_data)) > 0)
cat('There are', length(NAcol), 'columns with missing values in Training Set')

```

As suspected indeed there is some overlap of missing data among the features in the testing and training set. We will now clean the missing data.

Here's the percentage of missing data missing by variables. Variables with more than 15% missing data are removed from the model.
```{r}
aggr(combined_data,col=c('blue','red'),numbers=TRUE,sortVars=TRUE)
NAcombined=colSums(is.na(combined_data))/dim(combined_data)[1]
NAcombined=sort(subset(NAcombined,NAcombined>0),decreasing=TRUE)
plot(NAcombined,type="h",lwd=10,xaxt="n")
axis(1,at=1:length(NAcombined),labels=names(NAcombined))
cat("The variables that contain more than 50% missing data are",names(subset(NAcombined,NAcombined>0.15)))
combined_data_clean=subset(combined_data,select=-c(PoolQC,MiscFeature,Alley,Fence,FireplaceQu,LotFrontage))
```

Converting characters to factors for the imputation and modeling process.
```{r}
combined_data_clean=combined_data_clean%>%mutate_if(is.character,as.factor)
```
  
```{r}
temp_data=mice(subset(combined_data_clean,select=-SalePrice),m=1,maxit = 25,meth='pmm',seed=100)
summary(temp_data)
```

```{r}
combined_data_imputed=cbind(complete(temp_data,1),combined_data_clean$SalePrice)
colnames(combined_data_imputed)[colnames(combined_data_imputed)=="combined_data_clean$SalePrice"] <- "SalePrice"
```



Separating train and test sets from the combined set.
```{r}
train_imputed=subset(combined_data_imputed,combined_data_imputed$SalePrice!="NA")
test_imputed=subset(combined_data_imputed,is.na(combined_data_imputed$SalePrice))

combined_imputed=rbind(train_imputed,test_imputed)
test_imputed =select(test_imputed,-one_of("SalePrice"))
```

Let's check the training set for any missing values
```{r}
cat('There are', length(which(colSums(is.na(train_imputed)) > 0)), 'columns with missing values in Training Set')
cat("\n")
cat('There are', length(which(colSums(is.na(test_imputed)) > 0)), 'columns with missing values in Testing Set')

```

Find all the character features 
```{r}
library(tidyverse)
names(combined_imputed %>% select_if(negate(is.numeric)))
```

We will now transform the columns which are categorical.
```{r}
combined_imputed$MSSubClass = as.character(combined_imputed$MSSubClass)
combined_imputed$OverallCond = as.character(combined_imputed$OverallCond)
combined_imputed$YrSold = as.character(combined_imputed$YrSold)
combined_imputed$MoSold = as.character(combined_imputed$MoSold)
```

We will now perform label encoding for columns where the ordering is important

```{r}
library(caret)
cols = c("BsmtQual", "BsmtCond", "GarageQual", "GarageCond",
         "ExterQual", "ExterCond", "HeatingQC", "KitchenQual",
         "BsmtFinType1", "BsmtFinType2", "Functional", 
         "BsmtExposure", "GarageFinish", "LandSlope", "LotShape",
         "PavedDrive", "Street", "CentralAir", "MSSubClass",
         "OverallCond", "YrSold", "MoSold")

BsmtQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
ExterQual = c("Po", "Fa", "TA", "Gd", "Ex")
ExterCond = c("Po", "Fa", "TA", "Gd", "Ex")
HeatingQC = c("Po", "Fa", "TA", "Gd", "Ex")
KitchenQual = c("Po", "Fa", "TA", "Gd", "Ex")
BsmtFinType1 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
BsmtFinType2 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
Functional = c("Sal", "Sev", "Maj2", "Maj1", "Mod", "Min2", "Min1", "Typ")
BsmtExposure = c("None", "No", "Mn", "Av", "Gd")
GarageFinish = c("None", "Unf", "RFn", "Fin")
LandSlope = c("Sev", "Mod", "Gtl")
LotShape = c("IR3", "IR2", "IR1", "Reg")
PavedDrive = c("N", "P", "Y")
Street = c("Pave", "Grvl")
MSSubClass = c("20", "30", "40", "45", "50", "60", "70", "75", "80", "85", 
    "90", "120", "150", "160", "180", "190")
OverallCond = NA
MoSold = NA
YrSold = NA
CentralAir = NA
levels = list(BsmtQual, BsmtCond, GarageQual, GarageCond, ExterQual, 
              ExterCond, HeatingQC, KitchenQual, BsmtFinType1,
              BsmtFinType2, Functional, BsmtExposure, GarageFinish,
              LandSlope,LotShape, PavedDrive, Street, CentralAir,
              MSSubClass, OverallCond,YrSold, MoSold)
i = 1
for (c in cols) {
    if (c == "CentralAir" | c == "OverallCond" | c == "YrSold" | c == "MoSold") {
        combined_imputed[, c] = as.numeric(factor(combined_imputed[, c]))
    } else combined_imputed[, c] = as.numeric(factor(combined_imputed[, c], 
                                          levels = levels[[i]]))
    i = i + 1
}
```



Feature Engineering: Create a total surface area feature. 
```{r}
combined_imputed$TotalSF = combined_imputed$TotalBsmtSF
combined_imputed$X1stFlrSF + combined_imputed$X2ndFlrSF

```


Create Dummie Variable

```{r}
i <- sapply(combined_imputed, is.factor)
combined_imputed[i] <- lapply(combined_imputed[i], as.character)
```

```{r}
feature_classes <- sapply(names(combined_imputed), function(x) {
    class(combined_imputed[[x]])
})

```


```{r}

numeric_feats <- names(feature_classes[feature_classes != "character"])
print(numeric_feats)
```

```{r}

# get names of categorical features
categorical_feats <- names(feature_classes[feature_classes == "character"])

print(categorical_feats)
```

```{r}
# use caret dummyVars function for hot one encoding for categorical
# features
dummies <- dummyVars(~., combined_imputed[categorical_feats])
categorical_1_hot <- predict(dummies, combined_imputed[categorical_feats])

```


Fix the skewness in independent variables
```{r}
library(moments)
library(MASS)
skewed_feats <- sapply(numeric_feats, function(x) {
    skewness(combined_imputed[[x]], na.rm = TRUE)
})

## Keep only features that exceed a threshold (0.75) for skewness
skewed_feats <- skewed_feats[abs(skewed_feats) > 0.75]

## Transform skewed features with boxcox transformation
for (x in names(skewed_feats)) {
    bc = BoxCoxTrans(combined_imputed[[x]], lambda = 0.15)
    combined_imputed[[x]] = predict(bc, combined_imputed[[x]])
}

```


Final Preprocessed data for modeling 
```{r}
combined_imputed <- cbind(combined_imputed[numeric_feats], categorical_1_hot)
dim(combined_imputed)

training_processed <- combined_imputed[1:1460, ] # use in models
testing_processed<- combined_imputed[1461:2919, ] # use in models
```


We will now divide the training set further to create a validation set. 
```{r}
set.seed(4)
smp_siz = floor(0.75*nrow(training_processed))
train_ind = sample(seq_len(nrow(training_processed)),size = smp_siz)

train_subset =training_processed[train_ind,] #creates the training dataset with row numbers stored in train_ind
validation_set=training_processed[-train_ind,]#creates the test dataset excluding the row numbers mentioned in train_ind
```


Model Building

Simple Linear Model

```{r}
lm_model <- lm(train_subset$SalePrice ~ ., data=train_subset[, -51])
lm_prediction <- predict(lm_model, validation_set[, -51], type="response")
rmse(lm_prediction,validation_set$SalePrice)

```


Lasso Model
```{r}

set.seed(123)
cv_lasso = cv.glmnet(as.matrix(train_subset[, -51]), train_subset[, 51])

## Predictions
preds <- predict(cv_lasso, newx = as.matrix(validation_set[, -51]), s = "lambda.min")
rmse(validation_set$SalePrice, preds)
```


```{r}
library(e1071)
library(mlbench)

#linear.tune = tune.svm(train_subset$SalePrice~., data=train_subset[, -51], 
#                       kernel="linear", cost=c(0.001, 0.01, 0.1, 1,5,10))

#control <- trainControl(method="cv", number=10,verbose=TRUE)
#SVMgrid <- expand.grid( C = c(1.5,1.596,1.65,1.89,1.95,2,2.2,2.44))

#modelSvmRRB <- train(train_subset$SalePrice~., data=train_subset[, -51], method="svmLinear", trControl=control,tuneGrid = SVMgrid, verbose=FALSE)

model_svm<-tune(method = 'svm',train.x=train_subset[, -51],train.y=train_subset[,51],ranges = list( cost=c(0.001, 0.01, 0.1, 1,2,3,4,6,5,10,100)))


model_svm


best.linear = model_svm$best.model
tune.test = predict(best.linear, newdata=validation_set[,-51])
rmse(validation_set$SalePrice, tune.test)
```


```{r}
#svm_model<-svm(train_subset$SalePrice~., data=train_subset[, -51])
#svm_regressor = train(form = train_subset$SalePrice ~ ., data = train_subset[, -51])

model <- train(train_subset$SalePrice~., data=train_subset[, -51], method="lvq", preProcess="scale", trControl=control, tuneLength=5)



svm_model<-svm(train_subset$SalePrice~., data=train_subset[, -51])

svm_pred <- predict(svm_model,newdata = validation_set[, -51])

rmse(validation_set$SalePrice, svm_pred)

```


